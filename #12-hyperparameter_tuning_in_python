## CHAPTER 1: HYPERPARAMETERS AND PARAMETERS

Hyperparameters are something you set before the modeling process; not learned by algorithm.
coef_ contains the important information about coefficients on our variables in the model. We do not set this, it is learned by the algorithm through the modeling process.

a) Extracting a Logistic Regression parameter
# Create a list of original variable names from the training DataFrame
original_variables = X_train.columns

# Extract the coefficients of the logistic regression estimator
model_coefficients = log_reg_clf.coef_[0]

# Create a dataframe of the variables and coefficients & print it out
coefficient_df = pd.DataFrame({"Variable" : original_variables, "Coefficient": model_coefficients})
print(coefficient_df)

# Print out the top 3 positive variables
top_three_df = coefficient_df.sort_values(by="Coefficient", axis=0, ascending=False)[0:3]
print(top_three_df)
=> Output: <script.py> output:
           Variable  Coefficient
    0     LIMIT_BAL   -3.926e-06
    1           AGE   -3.170e-06
    2         PAY_0    2.189e-07
    3         PAY_2    1.129e-07
    4         PAY_3    1.110e-07
    5         PAY_4    1.264e-07
    6         PAY_5    1.291e-07
    7         PAY_6    1.235e-07
    8     BILL_AMT1   -7.001e-06
    9     BILL_AMT2   -4.343e-06
    10    BILL_AMT3    4.402e-06
    11    BILL_AMT4    1.599e-05
    12    BILL_AMT5    3.373e-06
    13    BILL_AMT6   -2.527e-06
    14     PAY_AMT1   -6.498e-05
    15     PAY_AMT2   -9.547e-05
    16     PAY_AMT3   -5.436e-05
    17     PAY_AMT4   -3.596e-05
    18     PAY_AMT5   -3.400e-05
    19     PAY_AMT6    3.083e-06
    20        SEX_2   -7.633e-08
    21  EDUCATION_1   -7.142e-09
    22  EDUCATION_2   -6.246e-08
    23  EDUCATION_3   -2.333e-08
    24  EDUCATION_4   -1.172e-09
    25  EDUCATION_5   -2.403e-09
    26  EDUCATION_6   -2.595e-10
    27   MARRIAGE_1   -2.480e-08
    28   MARRIAGE_2   -7.474e-08
    29   MARRIAGE_3    2.855e-09
         Variable  Coefficient
    11  BILL_AMT4    1.599e-05
    10  BILL_AMT3    4.402e-06
    12  BILL_AMT5    3.373e-06
The coefficients of the model allow you to see which variables are having a larger or smaller impact on the outcome. Additionally the sign lets you know if it is a positive or negative relationship.

b) Extracting a Random Forest parameter
# Extract the 7th (index 6) tree from the random forest
chosen_tree = rf_clf.estimators_[6]

# Visualize the graph using the provided image
imgplot = plt.imshow(tree_viz_image)
plt.show()

# Extract the parameters and level of the top (index 0) node
split_column = chosen_tree.tree_.feature[0]
split_column_name = X_train.columns[split_column]
split_value = chosen_tree.tree_.threshold[0]

# Print out the feature and level
print("This node split on feature {}, at a value of {}".format(split_column_name, split_value))
=> Output: <script.py> output:
    This node split on feature PAY_AMT4, at a value of 3869.5

c) Exploring Random Forest Hyperparameters
# Print out the old estimator, notice which hyperparameter is badly set
print(rf_clf_old)

# Get confusion matrix & accuracy for the old rf_model
print("Confusion Matrix: \n\n {} \n Accuracy Score: \n\n {}".format(
  confusion_matrix(y_test, rf_old_predictions),
  accuracy_score(y_test, rf_old_predictions))) 

# Create a new random forest classifier with better hyperparamaters
rf_clf_new = RandomForestClassifier(n_estimators=500)

# Fit this to the data and obtain predictions
rf_new_predictions = rf_clf_new.fit(X_train, y_train).predict(X_test)

# Assess the new model (using new predictions!)
print("Confusion Matrix: \n\n", confusion_matrix(y_test, rf_new_predictions))
print("Accuracy Score: \n\n", accuracy_score(y_test, rf_new_predictions))
=> Output: <script.py> output:
    RandomForestClassifier(n_estimators=5, random_state=42)
    Confusion Matrix: 
    
     [[276  37]
     [ 64  23]] 
     Accuracy Score: 
    
     0.7475
    Confusion Matrix: 
    
     [[300  13]
     [ 63  24]]
    Accuracy Score: 
    
     0.81

d) Hyperparameters of KNN
# Build a knn estimator for each value of n_neighbours
knn_5 = KNeighborsClassifier(n_neighbors=5)
knn_10 = KNeighborsClassifier(n_neighbors=10)
knn_20 = KNeighborsClassifier(n_neighbors=20)

# Fit each to the training data & produce predictions
knn_5_predictions = knn_5.fit(X_train, y_train).predict(X_test)
knn_10_predictions = knn_10.fit(X_train, y_train).predict(X_test)
knn_20_predictions = knn_20.fit(X_train, y_train).predict(X_test)

# Get an accuracy score for each of the models
knn_5_accuracy = accuracy_score(y_test, knn_5_predictions)
knn_10_accuracy = accuracy_score(y_test, knn_10_predictions)
knn_20_accuracy = accuracy_score(y_test, knn_20_predictions)
print("The accuracy of 5, 10, 20 neighbours was {}, {}, {}".format(knn_5_accuracy, knn_10_accuracy, knn_20_accuracy))
=> Output: <script.py> output:
    The accuracy of 5, 10, 20 neighbours was 0.7125, 0.765, 0.7825

e) Automating Hyperparameter Choice
Finding the best hyperparameter of interest without writing hundreds of lines of code for hundreds of models is an important efficiency gain that will greatly assist your future machine learning model building.
An important hyperparameter for the GBM algorithm is the learning rate. But which learning rate is best for this problem? By writing a loop to search through a number of possibilities, collating these and viewing them you can find the best one.
Possible learning rates to try include 0.001, 0.01, 0.05, 0.1, 0.2 and 0.5
You will have available X_train, X_test, y_train & y_test datasets, and GradientBoostingClassifier has been imported for you.

# Set the learning rates & results storage
learning_rates = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]
results_list = []

# Create the for loop to evaluate model predictions for each learning rate
for learning_rate in learning_rates:
    model = GradientBoostingClassifier(learning_rate=learning_rate)
    predictions = model.fit(X_train, y_train).predict(X_test)
    # Save the learning rate and accuracy score
    results_list.append([learning_rate, accuracy_score(y_test, predictions)])

# Gather everything into a DataFrame
results_df = pd.DataFrame(results_list, columns=['learning_rate', 'accuracy'])
print(results_df)
=> Output: <script.py> output:
       learning_rate  accuracy
    0          0.001     0.782
    1          0.010     0.802
    2          0.050     0.812
    3          0.100     0.797
    4          0.200     0.790
    5          0.500     0.777

f) Building Learning Curves
If we want to test many different values for a single hyperparameter it can be difficult to easily view that in the form of a DataFrame. Previously you learned about a nice trick to analyze this. A graph called a 'learning curve' can nicely demonstrate the effect of increasing or decreasing a particular hyperparameter on the final result.
Instead of testing only a few values for the learning rate, you will test many to easily see the effect of this hyperparameter across a large range of values. A useful function from NumPy is np.linspace(start, end, num) which allows you to create a number of values (num) evenly spread within an interval (start, end) that you specify.
You will have available X_train, X_test, y_train & y_test datasets.

# Set the learning rates & accuracies list
learn_rates = np.linspace(0.01, 2, num=30)
accuracies = []

# Create the for loop
for learn_rate in learn_rates:
  	# Create the model, predictions & save the accuracies as before
    model = GradientBoostingClassifier(learning_rate=learn_rate)
    predictions = model.fit(X_train, y_train).predict(X_test)
    accuracies.append(accuracy_score(y_test, predictions))

# Plot results    
plt.plot(learn_rates, accuracies)
plt.gca().set(xlabel='learning_rate', ylabel='Accuracy', title='Accuracy for different learning_rates')
plt.show() 


## CHAPTER 2: GRID SEARCH

a) Build Grid Search functions
# Create the function
def gbm_grid_search(learning_rate, max_depth):

	# Create the model
    model = GradientBoostingClassifier(learning_rate=learning_rate, max_depth=max_depth)
    
    # Use the model to make predictions
    predictions = model.fit(X_train, y_train).predict(X_test)
    
    # Return the hyperparameters and score
    return([learning_rate, max_depth, accuracy_score(y_test, predictions)])
You now have a function you can call to test different combinations of two hyperparameters for the GBM algorithm.

b) Iteratively tune multiple hyperparameters
1. Write a for-loop to test the values (0.01, 0.1, 0.5) for the learning_rate and (2, 4, 6) for the max_depth using the function you created gbm_grid_search and print the results.
# Create the relevant lists
results_list = []
learn_rate_list = [0.01, 0.1, 0.5]
max_depth_list = [2,4,6]

# Create the for loop
for learn_rate in learn_rate_list:
    for max_depth in max_depth_list:
        results_list.append(gbm_grid_search(learn_rate,max_depth))

# Print the results
print(results_list)   
=> Output: <script.py> output:
    [[0.01, 2, 0.78], [0.01, 4, 0.78], [0.01, 6, 0.76], [0.1, 2, 0.74], [0.1, 4, 0.76], [0.1, 6, 0.75], [0.5, 2, 0.73], [0.5, 4, 0.74], [0.5, 6, 0.74]]

2. Extend the gbm_grid_search function to include the hyperparameter subsample. Name this new function gbm_grid_search_extended.
# Extend the function input
def gbm_grid_search_extended(learn_rate, max_depth, subsample):

	# Extend the model creation section
    model = GradientBoostingClassifier(learning_rate=learn_rate, max_depth=max_depth, subsample=subsample)
    
    predictions = model.fit(X_train, y_train).predict(X_test)
    
    # Extend the return part
    return([learn_rate, max_depth, subsample, accuracy_score(y_test, predictions)])

3. Extend your loop to call gbm_grid_search (available in your console), then test the values [0.4 , 0.6] for the subsample hyperparameter and print the results. max_depth_list & learn_rate_list are available in your environment.
results_list = []

# Create the new list to test
subsample_list = [0.4, 0.6]

for learn_rate in learn_rate_list:
    for max_depth in max_depth_list:
    	
        # Extend the for loop
        for subsample in subsample_list:
        	
            # Extend the results to include the new hyperparameter
            results_list.append(gbm_grid_search_extended(learn_rate, max_depth, subsample))

# Print results
print(results_list)     
=> Output: <script.py> output:
    [[0.01, 2, 0.4, 0.73], [0.01, 2, 0.6, 0.74], [0.01, 4, 0.4, 0.73], [0.01, 4, 0.6, 0.76], [0.01, 6, 0.4, 0.72], [0.01, 6, 0.6, 0.78], [0.1, 2, 0.4, 0.76], [0.1, 2, 0.6, 0.75], [0.1, 4, 0.4, 0.72], [0.1, 4, 0.6, 0.73], [0.1, 6, 0.4, 0.73], [0.1, 6, 0.6, 0.71], [0.5, 2, 0.4, 0.65], [0.5, 2, 0.6, 0.67], [0.5, 4, 0.4, 0.72], [0.5, 4, 0.6, 0.68], [0.5, 6, 0.4, 0.69], [0.5, 6, 0.6, 0.72]]
You went from 2 to 3 hyperparameters and can see how you could extend that to even more values and hyperparameters.

c)
