## CHAPTER 1: INTRODUCTION TO APACHE SPARK AND PYSPARK

a) Creating a SparkSession

# Import SparkSession from pyspark.sql
from pyspark.sql import SparkSession

# Create my_spark
my_spark = SparkSession.builder.appName("my_spark").getOrCreate()

# Print my_spark
print(my_spark)
=> Output: <script.py> output:
    <pyspark.sql.session.SparkSession object at 0x7f63b4756280>

b) Loading census data

# Read in the CSV
census_adult = spark.read.csv("adult_reduced.csv")

# Show the DataFrame
census_adult.show()
=> Output: <script.py> output:
    +---+-------------+--------------+-----------------+--------+
    |_c0|          _c1|           _c2|              _c3|     _c4|
    +---+-------------+--------------+-----------------+--------+
    |age|education.num|marital.status|       occupation|  income|
    | 90|            9|       Widowed|                ?|50000.00|
    | 82|            9|       Widowed|  Exec-managerial|50000.00|
    | 66|           10|       Widowed|                ?|36526.00|
    | 54|            4|      Divorced|Machine-op-inspct|50000.00|
    | 41|           10|     Separated|   Prof-specialty|50000.00|
    | 34|            9|      Divorced|    Other-service|50000.00|
    | 38|            6|     Separated|     Adm-clerical|27800.00|
    | 74|           16| Never-married|   Prof-specialty|12400.00|
    | 68|            9|      Divorced|   Prof-specialty|75000.00|
    | 41|           10| Never-married|     Craft-repair|89000.00|
    | 45|           16|      Divorced|   Prof-specialty|24000.00|
    | 38|           15| Never-married|   Prof-specialty|25000.00|
    | 52|           13|       Widowed|    Other-service|83975.00|
    | 32|           14|     Separated|  Exec-managerial|85937.00|
    | 51|           16| Never-married|                ?| 9098.00|
    | 46|           15|      Divorced|   Prof-specialty|45690.00|
    | 45|            7|      Divorced| Transport-moving|32532.00|
    | 57|           14|      Divorced|  Exec-managerial|65000.00|
    | 22|           12| Never-married|Handlers-cleaners|12412.00|
    +---+-------------+--------------+-----------------+--------+
    only showing top 20 rows

c) Reading a CSV and performing aggregations

Note: Advantage of using PySpark over pandas for data processing on large datasets? PySpark can distribute data and computations across multiple nodes, enabling faster processing of large datasets.

You have a spreadsheet of Data Scientist salaries from companies ranging is size from small to large. You want to see if there is a major difference between average salaries grouped by company size.

# Load the CSV file into a DataFrame
salaries_df = spark.read.csv("salaries.csv", header=True, inferSchema=True)

# Count the total number of rows
row_count = salaries_df.count()
print(f"Total rows: {row_count}")

# Group by company size and calculate the average of salaries
salaries_df.groupBy("company_size").agg({"salary_in_usd": "avg"}).show()
salaries_df.show()
=> Output: <script.py> output:
    Total rows: 37234
    +------------+------------------+
    |company_size|avg(salary_in_usd)|
    +------------+------------------+
    |           L|147706.87870434183|
    |           M|161471.45763426356|
    |           S|          88036.29|
    +------------+------------------+
    
    +---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+
    |work_year|experience_level|employment_type|           job_title|salary|salary_currency|salary_in_usd|employee_residence|remote_ratio|company_location|company_size|
    +---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+
    |     2020|              EN|             FT| Azure Data Engineer|100000|            USD|       100000|                MU|           0|              MU|           S|
    |     2020|              EN|             CT|  Staff Data Analyst| 60000|            CAD|        44753|                CA|          50|              CA|           L|
    |     2020|              SE|             FT|Staff Data Scientist|164000|            USD|       164000|                US|          50|              US|           M|
    |     2020|              EN|             FT|        Data Analyst| 42000|            EUR|        47899|                DE|           0|              DE|           L|
    |     2020|              EX|             FT|      Data Scientist|300000|            USD|       300000|                US|         100|              US|           L|
    |     2020|              MI|             CT|  Sales Data Analyst| 60000|            USD|        60000|                NG|           0|              NG|           M|
    |     2020|              EX|             FT|  Staff Data Analyst| 15000|            USD|        15000|                NG|           0|              CA|           M|
    |     2020|              MI|             FT|Business Data Ana...| 95000|            USD|        95000|                US|           0|              US|           M|
    |     2020|              EN|             FT|        Data Analyst| 20000|            EUR|        22809|                PT|         100|              PT|           M|
    |     2020|              EN|             FT|      Data Scientist| 43200|            EUR|        49268|                DE|           0|              DE|           S|
    |     2020|              SE|             FT|Machine Learning ...|157000|            CAD|       117104|                CA|          50|              CA|           L|
    |     2020|              EN|             FT|       Data Engineer| 48000|            EUR|        54742|                PK|         100|              DE|           L|
    |     2020|              MI|             FT|Product Data Analyst| 20000|            USD|        20000|                HN|           0|              HN|           S|
    |     2020|              MI|             FT|       Data Engineer| 51999|            EUR|        59303|                DE|         100|              DE|           S|
    |     2020|              EN|             FT|   Big Data Engineer| 70000|            USD|        70000|                US|         100|              US|           L|
    |     2020|              SE|             FT|      Data Scientist| 60000|            EUR|        68428|                GR|         100|              US|           L|
    |     2020|              MI|             FT|  Research Scientist|450000|            USD|       450000|                US|           0|              US|           M|
    |     2020|              MI|             FT|        Data Analyst| 41000|            EUR|        46759|                FR|          50|              FR|           L|
    |     2020|              MI|             FT|       Data Engineer| 65000|            EUR|        74130|                AT|          50|              AT|           L|
    |     2020|              MI|             FT|      Data Scientist|103000|            USD|       103000|                US|         100|              US|           L|
    +---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+
    only showing top 20 rows


d) Filtering by company

Using that same dataset from the last exercise, you realized that you only care about the jobs that are entry level ("EN") in Canada ("CA"). What does the salaries look like there? Remember, there's already a SparkSession called spark in your workspace!

# Average salary for entry level in Canada
CA_jobs = ca_salaries_df.filter(ca_salaries_df['company_location'] == "CA").filter(ca_salaries_df['experience_level']
 == "EN").groupBy().avg("salary_in_usd")

# Show the result
CA_jobs.show()
=> Output: <script.py> output:
    +------------------+
    |avg(salary_in_usd)|
    +------------------+
    | 97330.53932584269|
    +------------------+

e) Infer and filter

Imagine you have a census dataset that you know has a header and a schema. Let's load that dataset and let PySpark infer the schema. What do you see if you filter on adults over 40?

# Load the dataframe
census_df = spark.read.json("adults.json")

# Filter rows based on age condition
salary_filtered_census = census_df.filter(census_df["age"] > 40)

# Show the result
salary_filtered_census.show()
=> Output: <script.py> output:
    +---+-------------+------+------------------+-----------------+
    |age|education.num|income|    marital.status|       occupation|
    +---+-------------+------+------------------+-----------------+
    | 90|            9| <=50K|           Widowed|                ?|
    | 82|            9| <=50K|           Widowed|  Exec-managerial|
    | 66|           10| <=50K|           Widowed|                ?|
    | 54|            4| <=50K|          Divorced|Machine-op-inspct|
    | 41|           10| <=50K|         Separated|   Prof-specialty|
    | 74|           16|  >50K|     Never-married|   Prof-specialty|
    | 68|            9| <=50K|          Divorced|   Prof-specialty|
    | 41|           10|  >50K|     Never-married|     Craft-repair|
    | 45|           16|  >50K|          Divorced|   Prof-specialty|
    | 52|           13|  >50K|           Widowed|    Other-service|
    | 51|           16|  >50K|     Never-married|                ?|
    | 46|           15|  >50K|          Divorced|   Prof-specialty|
    | 45|            7|  >50K|          Divorced| Transport-moving|
    | 57|           14|  >50K|          Divorced|  Exec-managerial|
    | 61|            9| <=50K|          Divorced|            Sales|
    | 51|           10| <=50K|Married-civ-spouse| Transport-moving|
    | 61|            9| <=50K|Married-civ-spouse|                ?|
    | 49|            3| <=50K|Married-civ-spouse|    Other-service|
    | 59|            6|  >50K|           Widowed|  Exec-managerial|
    | 52|           15|  >50K|          Divorced|  Exec-managerial|
    +---+-------------+------+------------------+-----------------+
    only showing top 20 rows

f) Schema writeout

from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# Fill in the schema with the columns you need from the exercise instructions
schema = StructType([StructField("age",IntegerType()),
                     StructField("education_num",IntegerType()),
                     StructField("marital_status",StringType()),
                     StructField("occupation",StringType()),
                     StructField("income",StringType())
                    ])

# Read in the CSV, using the schema you defined above
census_adult = spark.read.csv("adult_reduced_100.csv", sep=',', header=False, schema=schema)

# Print out the schema
census_adult.printSchema()
=> Output: <script.py> output:
    root
     |-- age: integer (nullable = true)
     |-- education_num: integer (nullable = true)
     |-- marital_status: string (nullable = true)
     |-- occupation: string (nullable = true)
     |-- income: string (nullable = true)


## CHAPTER 2: PYSPARK IN PYTHON

a) Handling missing data with fill and drop

# Drop rows with any nulls
census_cleaned = census_df.na.drop()

# Show the result
census_cleaned.show()
=> Output: <script.py> output:
    +---+-------------+------------------+-----------------+------+
    |_c0|          _c1|               _c2|              _c3|   _c4|
    +---+-------------+------------------+-----------------+------+
    |age|education.num|    marital.status|       occupation|income|
    | 82|            9|           Widowed|  Exec-managerial| <=50K|
    | 54|            4|          Divorced|Machine-op-inspct| <=50K|
    | 38|            6|         Separated|     Adm-clerical| <=50K|
    | 68|            9|          Divorced|   Prof-specialty| <=50K|
    | 41|           10|     Never-married|     Craft-repair|  >50K|
    | 45|           16|          Divorced|   Prof-specialty|  >50K|
    | 38|           15|     Never-married|   Prof-specialty|  >50K|
    | 52|           13|           Widowed|    Other-service|  >50K|
    | 32|           14|         Separated|  Exec-managerial|  >50K|
    | 45|            7|          Divorced| Transport-moving|  >50K|
    | 57|           14|          Divorced|  Exec-managerial|  >50K|
    | 34|           13|         Separated|            Sales|  >50K|
    | 37|           13|     Never-married|  Exec-managerial|  >50K|
    | 29|            7|         Separated|            Sales| <=50K|
    | 61|            9|          Divorced|            Sales| <=50K|
    | 51|           10|Married-civ-spouse| Transport-moving| <=50K|
    | 21|           11|Married-civ-spouse|     Craft-repair| <=50K|
    | 33|            2|Married-civ-spouse|     Craft-repair| <=50K|
    | 49|            3|Married-civ-spouse|    Other-service| <=50K|
    +---+-------------+------------------+-----------------+------+
    only showing top 20 rows

b) Column operations - creating and renaming columns

The census dataset is still not quite showing everything you want it to. Let's make a new synthetic column by adding a new column based on existing columns, and rename it for clarity.

# Create a new column 'weekly_salary'
census_df_weekly = census_df.withColumn("weekly_salary", census_df.income / 52)

# Rename the 'age' column to 'years'
census_df_weekly = census_df_weekly.withColumnRenamed("age", "years")

# Show the result
census_df_weekly.show()
=> Output: <script.py> output:
    +-----+-------------+--------------+-----------------+-------+------------------+
    |years|education.num|marital.status|       occupation| income|     weekly_salary|
    +-----+-------------+--------------+-----------------+-------+------------------+
    |   90|            9|       Widowed|                ?|50000.0| 961.5384615384615|
    |   82|            9|       Widowed|  Exec-managerial|50000.0| 961.5384615384615|
    |   66|           10|       Widowed|                ?|36526.0| 702.4230769230769|
    |   54|            4|      Divorced|Machine-op-inspct|50000.0| 961.5384615384615|
    |   41|           10|     Separated|   Prof-specialty|50000.0| 961.5384615384615|
    |   34|            9|      Divorced|    Other-service|50000.0| 961.5384615384615|
    |   38|            6|     Separated|     Adm-clerical|27800.0| 534.6153846153846|
    |   74|           16| Never-married|   Prof-specialty|12400.0|238.46153846153845|
    |   68|            9|      Divorced|   Prof-specialty|75000.0|1442.3076923076924|
    |   41|           10| Never-married|     Craft-repair|89000.0|1711.5384615384614|
    |   45|           16|      Divorced|   Prof-specialty|24000.0|461.53846153846155|
    |   38|           15| Never-married|   Prof-specialty|25000.0| 480.7692307692308|
    |   52|           13|       Widowed|    Other-service|83975.0|1614.9038461538462|
    |   32|           14|     Separated|  Exec-managerial|85937.0|1652.6346153846155|
    |   51|           16| Never-married|                ?| 9098.0|174.96153846153845|
    |   46|           15|      Divorced|   Prof-specialty|45690.0| 878.6538461538462|
    |   45|            7|      Divorced| Transport-moving|32532.0| 625.6153846153846|
    |   57|           14|      Divorced|  Exec-managerial|65000.0|            1250.0|
    |   22|           12| Never-married|Handlers-cleaners|12412.0|238.69230769230768|
    |   34|           13|     Separated|            Sales|50000.0| 961.5384615384615|
    +-----+-------------+--------------+-----------------+-------+------------------+
    only showing top 20 rows

c) Joining flights with their destination airports

You've been hired as a data engineer for a global travel company. Your first task is to help the company improve its operations by analyzing flight data. You have two datasets in your workspace: one containing details about flights (flights) and another with information about destination airports (airports), both are already available in your workspace.

# Examine the data
airports.show()

# .withColumnRenamed() renames the "faa" column to "dest" 
airports = airports.withColumnRenamed("faa", "dest")

# Join the DataFrames
flights_with_airports = flights.join(airports, on="dest", how="leftouter")

# Examine the new DataFrame
flights_with_airports.show()
=> Output: <script.py> output:
    +---+--------------------+----------------+-----------------+----+---+---+
    |faa|                name|             lat|              lon| alt| tz|dst|
    +---+--------------------+----------------+-----------------+----+---+---+
    |04G|   Lansdowne Airport|      41.1304722|      -80.6195833|1044| -5|  A|
    |06A|Moton Field Munic...|      32.4605722|      -85.6800278| 264| -5|  A|
    |06C| Schaumburg Regional|      41.9893408|      -88.1012428| 801| -6|  A|
    |06N|     Randall Airport|       41.431912|      -74.3915611| 523| -5|  A|
    |09J|Jekyll Island Air...|      31.0744722|      -81.4277778|  11| -4|  A|
    |0A9|Elizabethton Muni...|      36.3712222|      -82.1734167|1593| -4|  A|
    |0G6|Williams County A...|      41.4673056|      -84.5067778| 730| -5|  A|
    |0G7|Finger Lakes Regi...|      42.8835647|      -76.7812318| 492| -5|  A|
    |0P2|Shoestring Aviati...|      39.7948244|      -76.6471914|1000| -5|  U|
    |0S9|Jefferson County ...|      48.0538086|     -122.8106436| 108| -8|  A|
    |0W3|Harford County Ai...|      39.5668378|      -76.2024028| 409| -5|  A|
    |10C|  Galt Field Airport|      42.4028889|      -88.3751111| 875| -6|  U|
    |17G|Port Bucyrus-Craw...|      40.7815556|      -82.9748056|1003| -5|  A|
    |19A|Jackson County Ai...|      34.1758638|      -83.5615972| 951| -4|  U|
    |1A3|Martin Campbell F...|      35.0158056|      -84.3468333|1789| -4|  A|
    |1B9| Mansfield Municipal|      42.0001331|      -71.1967714| 122| -5|  A|
    |1C9|Frazier Lake Airpark|54.0133333333333|-124.768333333333| 152| -8|  A|
    |1CS|Clow Internationa...|      41.6959744|      -88.1292306| 670| -6|  U|
    |1G3|  Kent State Airport|      41.1513889|      -81.4151111|1134| -4|  A|
    |1OH|     Fortman Airport|      40.5553253|      -84.3866186| 885| -5|  U|
    +---+--------------------+----------------+-----------------+----+---+---+
    only showing top 20 rows
    
    +----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+--------------------+---------+-----------+----+---+---+
    |dest|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|air_time|distance|hour|minute|                name|      lat|        lon| alt| tz|dst|
    +----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+--------------------+---------+-----------+----+---+---+
    | LAX|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA|     132|     954|   6|    58|    Los Angeles Intl|33.942536|-118.408075| 126| -8|  A|
    | HNL|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA|     360|    2677|  10|    40|       Honolulu Intl|21.318681|-157.922428|  13|-10|  N|
    | SFO|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA|     111|     679|  14|    43|  San Francisco Intl|37.618972|-122.374889|  13| -8|  A|
    | SJC|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX|      83|     569|  17|     5|Norman Y Mineta S...|  37.3626|-121.929022|  62| -8|  A|
    | BUR|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA|     127|     937|   7|    54|            Bob Hope|34.200667|-118.358667| 778| -8|  A|
    | DEN|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX|     121|     991|  10|    37|         Denver Intl|39.861656|-104.673178|5431| -7|  A|
    | OAK|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX|      90|     543|   8|    47|Metropolitan Oakl...|37.721278|-122.220722|   9| -8|  A|
    | SFO|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA|      98|     679|  16|    55|  San Francisco Intl|37.618972|-122.374889|  13| -8|  A|
    | SAN|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA|     135|    1050|  12|    36|      San Diego Intl|32.733556|-117.189667|  17| -8|  A|
    | ORD|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA|     198|    1721|  18|    12|  Chicago Ohare Intl|41.978603| -87.904842| 668| -6|  A|
    | LAX|2014|   11|  8|    1653|       -2|    1924|       -1|     AS| N323AS|   448|   SEA|     130|     954|  16|    53|    Los Angeles Intl|33.942536|-118.408075| 126| -8|  A|
    | PHX|2014|    8|  3|    1120|        0|    1415|        2|     AS| N305AS|   656|   SEA|     154|    1107|  11|    20|Phoenix Sky Harbo...|33.434278|-112.011583|1135| -7|  N|
    | LAS|2014|   10| 30|     811|       21|    1038|       29|     AS| N433AS|   608|   SEA|     127|     867|   8|    11|      Mc Carran Intl|36.080056| -115.15225|2141| -8|  A|
    | ANC|2014|   11| 12|    2346|       -4|     217|      -28|     AS| N765AS|   121|   SEA|     183|    1448|  23|    46|Ted Stevens Ancho...|61.174361|-149.996361| 152| -9|  A|
    | SFO|2014|   10| 31|    1314|       89|    1544|      111|     AS| N713AS|   306|   SEA|     129|     679|  13|    14|  San Francisco Intl|37.618972|-122.374889|  13| -8|  A|
    | SFO|2014|    1| 29|    2009|        3|    2159|        9|     UA| N27205|  1458|   PDX|      90|     550|  20|     9|  San Francisco Intl|37.618972|-122.374889|  13| -8|  A|
    | SMF|2014|   12| 17|    2015|       50|    2150|       41|     AS| N626AS|   368|   SEA|      76|     605|  20|    15|     Sacramento Intl|38.695417|-121.590778|  27| -8|  A|
    | MDW|2014|    8| 11|    1017|       -3|    1613|       -7|     WN| N8634A|   827|   SEA|     216|    1733|  10|    17| Chicago Midway Intl|41.785972| -87.752417| 620| -6|  A|
    | BOS|2014|    1| 13|    2156|       -9|     607|      -15|     AS| N597AS|    24|   SEA|     290|    2496|  21|    56|General Edward La...|42.364347| -71.005181|  19| -5|  A|
    | BUR|2014|    6|  5|    1733|      -12|    1945|      -10|     OO| N215AG|  3488|   PDX|     111|     817|  17|    33|            Bob Hope|34.200667|-118.358667| 778| -8|  A|
    +----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+--------------------+---------+-----------+----+---+---+
    only showing top 20 rows

d) Integers in PySpark UDFs

UDF (User Defined Function): custom function to work with data using Pyspark dataframes
PySpark UDF = Best for relatively small datasets, simple transformations like data cleaning, works at column level, must be registered in Spark Session.
Pandas UDF = Suitable for large datasets, supports complex operations beyond cleaning, works at row level, can be called outside Spark Session.

# Register the function age_category as a UDF
age_category_udf = udf(age_category, StringType())

# Apply your udf to the DataFrame
age_category_df_2 = age_category_df.withColumn("category", age_category_udf(age_category_df["age"]))

# Show df
age_category_df_2.show()
=> Output: <script.py> output:
    +-------+---+--------+
    |   name|age|category|
    +-------+---+--------+
    |  Alice| 25|   Adult|
    |    Bob| 17|   Child|
    |Charlie| 65|  Senior|
    |  David| 34|   Adult|
    +-------+---+--------+

e) Pandas UDFs

# Define a Pandas UDF that adds 10 to each element in a vectorized way
@pandas_udf(DoubleType())
def add_ten_pandas(column):
    return column + 10

# Apply the UDF and show the result
df.withColumn("10_plus", add_ten_pandas(df["value"]))
df.show()
=> Output: <script.py> output:
    +-----+
    |value|
    +-----+
    |  1.0|
    |  2.0|
    |  3.0|
    +-----+


## CHAPTER 3: INTRODUCTION TO PYSPARK SQL

RDDs (Resilient Distributed Datasets) are distributed, fault-tolerant data collections across a cluster, ideal for handling large-scale data.
They are immutable and can be transformed with operations like map() or filter(), and evaluated using actions like collect() or parallelize().

DataFrames: High-level, easier to use, support SQL-like operations with less code, and include schema information (columns, data types).
RDDs: Low-level, more flexible but require more code for complex operations.
Type & Schema: RDDs preserve data types but lack schema awareness, making structured/SQL-style data handling harder.
Scaling & Analytics: RDDs scale well but are verbose and less efficient for analytics compared to DataFrames.

a) Creating RDDs

# Create a DataFrame
df = spark.read.csv("salaries.csv", header=True, inferSchema=True)

# Convert DataFrame to RDD
rdd = df.rdd

# Show the RDD's contents
rdd.collect()
print(rdd)
=> Output: <script.py> output:
    MapPartitionsRDD[14] at javaToPython at NativeMethodAccessorImpl.java:0

b) Collecting RDDs

For this exercise, you’ll work with both RDDs and DataFrames in PySpark. The goal is to group data and perform aggregation using both RDD operations and DataFrame methods. You will load a CSV file containing employee salary data into PySpark as an RDD. You'll then group by the experience level data and calculate the maximum salary for each experience level from a DataFrame. By doing this, you'll see the relative strengths of both data formats.

# Create an RDD from the df_salaries
rdd_salaries = df_salaries.rdd

# Collect and print the results
print(rdd_salaries.collect()) 

# Group by the experience level and calculate the maximum salary
dataframe_results = df_salaries.groupby("experience_level").agg({"salary_in_usd": 'max'})

# Show the results
dataframe_results.show()  
=> Output: <script.py> output:
    |experience_level|max(salary_in_usd)|
    +----------------+------------------+
    |              EX|            325000|
    |              MI|            450000|
    |              EN|            250000|
    |              SE|            412000|
    +----------------+------------------+

c) Querying on a temp view

In this exercise, you'll practice registering a DataFrame as a temporary SQL view in PySpark. Temporary views are powerful tools that allow you to query data using SQL syntax, making complex data manipulations easier and more intuitive. Your goal is to create a view from a provided DataFrame and run SQL queries against it, a common task for ETL and ELT work.

# Register as a view
df.createOrReplaceTempView("data_view")

# Advanced SQL query: Calculate total salary by Position
result = spark.sql("""
    SELECT Position, SUM(Salary) AS Total_Salary
    FROM data_view
    GROUP BY Position
    ORDER BY Total_Salary DESC
    """
)

result.show()
=> Output: <script.py> output:
    +-----------+------------+
    |   Position|Total_Salary|
    +-----------+------------+
    |   Engineer|     2544679|
    |Coordinator|     1752702|
    |   Director|     1702050|
    | Specialist|     1479329|
    |    Manager|     1475810|
    |    Analyst|     1137642|
    +-----------+------------+

d) Running SQL on DataFrames

DataFrames can be easily manipulated using SQL queries in PySpark. The .sql() method in a SparkSession enables applications to run SQL queries programmatically and returns the result as another DataFrame. In this exercise, you'll create a temporary table of a DataFrame that you have created previously, then construct a query to select the names of the people from the temporary table and assign the result to a new DataFrame.

# Create a temporary table "people"
df.createOrReplaceTempView("people")

# Select the names from the temporary table people
query = """SELECT name FROM people"""

# Assign the result of Spark's query to people_df_names
people_df_names = spark.sql(query)

# Print the top 10 names of the people
people_df_names.show(10)
=> <script.py> output:
    +-----+
    | name|
    +-----+
    |Alice|
    |  Bob|
    |Cathy|
    +-----+

e) Analytics with SQL on DataFrames

# Create a temporary view of salaries_table
salaries_df.createOrReplaceTempView('salaries_table')

# Construct the "query"
query = '''SELECT job_title, salary_in_usd FROM salaries_table WHERE company_location == "CA"'''

# Apply the SQL "query"
canada_titles = spark.sql(query)

# Generate basic statistics
canada_titles.describe().show()
=> <script.py> output:
    +-------+--------------------+-----------------+
    |summary|           job_title|    salary_in_usd|
    +-------+--------------------+-----------------+
    |  count|                1115|             1115|
    |   mean|                null|143228.7273542601|
    | stddev|                null|61839.75996920976|
    |    min|        AI Architect|            15000|
    |    max|Statistical Progr...|           800000|
    +-------+--------------------+-----------------+

f) Aggregating in PySpark

# Find the minimum salaries for small companies
salaries_df.filter(salaries_df.company_size == "S").groupBy().min("salary_in_usd").show()

# Find the maximum salaries for large companies
salaries_df.filter(salaries_df.company_size == "L").groupBy().max("salary_in_usd").show()
=> Output: <script.py> output:
    +------------------+
    |min(salary_in_usd)|
    +------------------+
    |             15809|
    +------------------+
    
    +------------------+
    |max(salary_in_usd)|
    +------------------+
    |            423000|
    +------------------+

g) Aggregating in RDDs

# DataFrame Creation
data = [("HR", "3000"), ("IT", "4000"), ("Finance", "3500")]
columns = ["Department", "Salary"]
df = spark.createDataFrame(data, schema=columns)

# Map the DataFrame to an RDD
rdd = df.rdd.map(lambda row: (row["Department"], row["Salary"]))

# Apply a lambda function to get the sum of the DataFrame
rdd_aggregated = rdd.reduceByKey(lambda x, y: x + y)

# Show the collected Results
print(rdd_aggregated.collect())
=> Output: <script.py> output:
    [('HR', '3000'), ('IT', '4000'), ('Finance', '3500')]

h) Complex Aggregations

# Average salaries at large US companies
salaries_df.filter(salaries_df.company_size == "L").filter(salaries_df.company_location == "US").groupBy().avg("salary_in_usd").show()

# Set a large companies variable for other analytics
large_companies=salaries_df.filter(salaries_df.company_size == "L").filter(salaries_df.company_location == "US")

# Total salaries in usd
large_companies.groupBy().sum("salary_in_usd").show()
=> Output: <script.py> output:
    +------------------+
    |avg(salary_in_usd)|
    +------------------+
    |165465.82197614992|
    +------------------+
    
    +------------------+
    |sum(salary_in_usd)|
    +------------------+
    |         194256875|
    +------------------+

i) Bringing it all together I

You've built a solid foundation in PySpark, explored its core components, and worked through practical scenarios involving Spark SQL, DataFrames, and advanced operations. Now it’s time to bring it all together. Over the next two exercises, you're going to make a SparkSession, a Dataframe, cache that Dataframe, conduct analytics and explain the outcome!

# Import SparkSession from pyspark.sql
from pyspark.sql import SparkSession

# Create my_spark
my_spark = SparkSession.builder.appName("final_spark").getOrCreate()

# Print my_spark
print(my_spark)

# Load dataset into a DataFrame
df = my_spark.createDataFrame(data, schema=columns)

df.show()
=> Output: <script.py> output:
    <pyspark.sql.session.SparkSession object at 0x7f0f77362640>
    +-----+----------+------+
    | Name|Department|Salary|
    +-----+----------+------+
    |Alice|        HR|  3000|
    |  Bob|        IT|  4000|
    |Cathy|        HR|  3500|
    +-----+----------+------+

j) Bringing it all together II

Create a DataFrame, apply transformations, cache it, and check if it’s cached. Then, uncache it to release memory. For this exercise a spark session has been made for you! Look carefully at the outcome of the .explain() method to understand what the outcome is!

# Cache the DataFrame
df.cache()

# Perform aggregation
agg_result = df.groupBy("Department").sum("Salary")
agg_result.show()

# Analyze the execution plan
agg_result.explain()

# Uncache the DataFrame
df.unpersist()
=> Output: <script.py> output:
    +----------+-----------+
    |Department|sum(Salary)|
    +----------+-----------+
    |        HR|       6500|
    |        IT|       4000|
    +----------+-----------+
    
    == Physical Plan ==
    AdaptiveSparkPlan isFinalPlan=false
    +- HashAggregate(keys=[Department#1], functions=[sum(Salary#2L)])
       +- Exchange hashpartitioning(Department#1, 200), ENSURE_REQUIREMENTS, [id=#60]
          +- HashAggregate(keys=[Department#1], functions=[partial_sum(Salary#2L)])
             +- InMemoryTableScan [Department#1, Salary#2L]
                   +- InMemoryRelation [Name#0, Department#1, Salary#2L], StorageLevel(disk, memory, deserialized, 1 replicas)
                         +- *(1) Scan ExistingRDD[Name#0,Department#1,Salary#2L]
