## CHAPTER 1: REGULAR EXPRESSIONS & WORD TOKENIZATION

* Natural Language Processing: Field of study focused on making sense of language (using statistics and computers) including topic idenfication and text classification
Applications: Chatbots, Translation, Sentiment Analysis, etc
* Regular Expressions: Strings with a special syntax, allow us to match patterns in other strings
Applications: Find web links in a document, Parse email addresses, Remove/replace unwanted characters
* Tokenization: Turning a string or document into tokens (small chunks)
Examples: Breaking out words or sentences, Separating Punctuation, Separating hashtags in a tweet
Purpose: Easier to map of speech, Matching common words, Removing unwanted tokens

a) Practicing regular expressions: re.split() and re.findall()
Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at my_string first by printing it in the IPython Shell, to determine how you might best match the different steps.
Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, "\n" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string "\n" - that is, the character "\" followed by the character "n" - and not as a new line.
The regular expression module re has already been imported for you.
Remember from the video that the syntax for the regex library is to always to pass the pattern first, and then the string second.

# Write a pattern to match sentence endings: sentence_endings
sentence_endings = r"[.?!]"

# Split my_string on sentence endings and print the result
print(re.split(sentence_endings, my_string))

# Find all capitalized words in my_string and print the result
capitalized_words = r"[A-Z]\w+"
print(re.findall(capitalized_words, my_string))

# Split my_string on spaces and print the result
spaces = r"\s+"
print(re.split(spaces, my_string))

# Find all digits in my_string and print the result
digits = r"\d+"
print(re.findall(digits, my_string))
=> Output: <script.py> output:
    ["Let's write RegEx", "  Won't that be fun", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']
    ['Let', 'RegEx', 'Won', 'Can', 'Or']
    ["Let's", 'write', 'RegEx!', "Won't", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']
    ['4', '19']

b) Word tokenization with NLTK
Your job in this exercise is to utilize word_tokenize and sent_tokenize from nltk.tokenize to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail.

# Import necessary modules
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize

# Split scene_one into sentences: sentences
sentences = sent_tokenize(scene_one)

# Use word_tokenize to tokenize the fourth sentence: tokenized_sent
tokenized_sent = word_tokenize(sentences[3])

# Make a set of unique tokens in the entire scene: unique_tokens
unique_tokens = set(word_tokenize(scene_one))

# Print the unique tokens result
print(unique_tokens)
=> Output: <script.py> output:
    {']', 'swallow', 'are', 'simple', 'maybe', 'Supposing', 'SCENE', 'will', 'be', 'is', 'ARTHUR', 'one', 'at', 'trusty', 'King', 'five', "n't", 'Camelot', "'em", 'them', 'husk', 'he', 'through', 'line', 'forty-three', 'all', 'bangin', 'your', 'Not', 'grip', 'sovereign', 'The', 'climes', 'does', 'Listen', 'question', 'needs', 'ridden', 'agree', 'to', 'That', 'by', 'It', 'goes', 'mean', 'times', 'Oh', '...', 'SOLDIER', 'winter', 'my', 'yeah', 'you', '1', 'they', 'African', 'use', 'Am', 'wind', 'So', 'other', 'in', 'wings', 'snows', 'Please', 'master', 'go', 'warmer', 'together', 'these', 'They', 'A', 'not', 'breadth', 'tell', "'m", 'ratios', 'the', 'minute', 'maintain', 'strangers', 'martin', 'plover', 'get', 'speak', 'wants', 'length', 'interested', 'a', 'that', 'temperate', 'Yes', 'matter', "'d", 'bring', 'join', 'using', 'under', 'sun', ':', ',', 'from', 'Will', 'back', "'s", 'search', 'covered', 'got', 'carry', 'lord', 'ask', 'defeator', 'Halt', 'two', 'velocity', 'weight', 'its', 'south', 'land', 'ounce', 'son', 'our', 'but', 'Uther', 'course', 'this', 'Pendragon', 'grips', '#', 'must', 'strand', 'held', 'coconuts', 'yet', 'You', 'horse', 'We', 'point', 'just', 'Saxons', 'court', 'fly', 'of', 'who', "'ve", 'Where', 'zone', 'anyway', 'I', 'every', 'Britons', 'second', 'European', 'Well', 'clop', 'if', 'Ridden', 'Arthur', 'dorsal', 'suggesting', 'knights', "'re", 'Are', 'where', 'Whoa', 'Pull', 'bird', 'me', '!', "'", 'Found', 'KING', 'house', 'feathers', 'beat', 'then', 'kingdom', 'why', 'empty', 'air-speed', 'But', 'migrate', 'there', 'coconut', 'Mercea', 'here', 'pound', 'carrying', 'could', 'No', '.', 'since', 'creeper', 'it', 'Patsy', 'have', 'Who', '2', 'order', 'Wait', 'In', 'tropical', 'guiding', '--', 'on', 'found', '?', 'right', 'am', 'servant', 'seek', 'an', 'non-migratory', 'or', 'and', 'What', 'castle', 'do', 'swallows', 'may', 'Court', 'England', 'carried', '[', 'with', 'halves'}

c) More regex with re.search()
In this exercise, you'll utilize re.search() and re.match() to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the nltk corpora. You have both scene_one and sentences available from the last exercise; now you can use them with re.search() and re.match() to extract and match more text.

# Search for the first occurrence of "coconuts" in scene_one: match
match = re.search("coconuts", scene_one)

# Print the start and end indexes of match
print(match.start(), match.end())
=> Output: <script.py> output:
    580 588

# Write a regular expression to search for anything in square brackets: pattern1
pattern1 = r"\[.*\]"

# Use re.search to find the first text in square brackets
print(re.search(pattern1, scene_one))
=> Output: <script.py> output:
    <re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>

# Find the script notation at the beginning of the fourth sentence and print it
pattern2 = r"[\w\s]+:"
print(re.match(pattern2, sentences[3]))
=> Output: <script.py> output:
    <re.Match object; span=(0, 7), match='ARTHUR:'>

d) Regex with NLTK tokenization
Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer class gives you some extra methods and attributes for parsing tweets. Here, you're given some example tweets to parse using both TweetTokenizer and regexp_tokenize from the nltk.tokenize module. These example tweets have been pre-loaded into the variable tweets. Feel free to explore it in the IPython Shell! Unlike the syntax for the regex library, with nltk_tokenize() you pass the pattern as the second argument.

# Import the necessary modules
from nltk.tokenize import regexp_tokenize
from nltk.tokenize import TweetTokenizer

# Define a regex pattern to find hashtags: pattern1
pattern1 = r"#\w+"
# Use the pattern on the first tweet in the tweets list
hashtags = regexp_tokenize(tweets[0], pattern1)
print(hashtags)
=> Output: <script.py> output:
    ['#nlp', '#python']

# Write a pattern that matches both mentions (@) and hashtags
pattern2 = r"([@#]\w+)"
# Use the pattern on the last tweet in the tweets list
mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)
print(mentions_hashtags)
=> Output: <script.py> output:
    ['@datacamp', '#nlp', '#python']

# Use the TweetTokenizer to tokenize all tweets into one list
tknzr = TweetTokenizer()
all_tokens = [tknzr.tokenize(t) for t in tweets]
print(all_tokens)
=> Output: <script.py> output:
    [['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]

e) Non-ascii tokenization
Here, you have access to a string called german_text, which has been printed for you in the Shell. Notice the emoji and the German characters!
The following modules have been pre-imported from nltk.tokenize: regexp_tokenize and word_tokenize.
Unicode ranges for emoji are:
('\U0001F300'-'\U0001F5FF'), ('\U0001F600-\U0001F64F'), ('\U0001F680-\U0001F6FF'), and ('\u2600'-\u26FF-\u2700-\u27BF').

# Tokenize and print all words in german_text
all_words = word_tokenize(german_text)
print(all_words)

# Tokenize and print only capital words
capital_words = r"[A-ZÜ]\w+"
print(regexp_tokenize(german_text, capital_words))

# Tokenize and print only emoji
emoji = "['\U0001F300-\U0001F5FF'|'\U0001F600-\U0001F64F'|'\U0001F680-\U0001F6FF'|'\u2600-\u26FF\u2700-\u27BF']"
print(regexp_tokenize(german_text, emoji))
=> Output: Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕

<script.py> output:
    ['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']
    ['Wann', 'Pizza', 'Und', 'Über']
    ['🍕', '🚕']

f) Charting practice
Try using your new skills to find and chart the number of words per line in the script using matplotlib. The Holy Grail script is loaded for you, and you need to use regex to find the words per line. Using list comprehensions here will speed up your computations. For example: my_lines = [tokenize(l) for l in lines] will call a function tokenize on each line in the list lines. The new transformed list will be saved in the my_lines variable.

# Split the script into lines: lines
lines = holy_grail.split('\n')

# Replace all script lines for speaker
pattern = "[A-Z]{2,}(\s)?(#\d)?([A-Z]{2,})?:"
lines = [re.sub(pattern, '', l) for l in lines]

# Tokenize each line: tokenized_lines
tokenized_lines = [regexp_tokenize(s, "\w+") for s in lines]

# Make a frequency list of lengths: line_num_words
line_num_words = [len(t_line) for t_line in tokenized_lines]

# Plot a histogram of the line lengths
plt.hist(line_num_words)

# Show the plot
plt.show()


## CHAPTER 2: SIMPLE TOPIC IDENTIFICATION

a) Building a Counter with bag-of-words
In this exercise, you'll build your first (in this course) bag-of-words counter using a Wikipedia article, which has been pre-loaded as article. Try doing the bag-of-words without looking at the full article text, and guessing what the topic is! If you'd like to peek at the title at the end, we've included it as article_title. Note that this article text has had very little preprocessing from the raw Wikipedia database entry. word_tokenize has been imported for you.

# Import Counter
from collections import Counter

# Tokenize the article: tokens
tokens = word_tokenize(article)

# Convert the tokens into lowercase: lower_tokens
lower_tokens = [t.lower() for t in tokens]

# Create a Counter with the lowercase tokens: bow_simple
bow_simple = Counter(lower_tokens)

# Print the 10 most common tokens
print(bow_simple.most_common(10))
=> Output: <script.py> output:
    [(',', 151), ('the', 150), ('.', 89), ('of', 81), ("''", 66), ('to', 63), ('a', 60), ('``', 47), ('in', 44), ('and', 41)]

b) Text preprocessing practice
# Import WordNetLemmatizer
from nltk.stem import WordNetLemmatizer

# Retain alphabetic words: alpha_only
alpha_only = [t for t in lower_tokens if t.isalpha()]

# Remove all stop words: no_stops
no_stops = [t for t in alpha_only if t not in english_stops]

# Instantiate the WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

# Lemmatize all tokens into a new list: lemmatized
lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]

# Create the bag-of-words: bow
bow = Counter(lemmatized)

# Print the 10 most common tokens
print(bow.most_common(10))
=> Output: <script.py> output:
    [('debugging', 40), ('system', 25), ('bug', 17), ('software', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('debugger', 13)]

c) Creating and querying a corpus with gensim
You'll use these data structures to investigate word trends and potential interesting topics in your document set. To get started, we have imported a few additional messy articles from Wikipedia, which were preprocessed by lowercasing all words, tokenizing them, and removing stop words and punctuation. These were then stored in a list of document tokens called articles. You'll need to do some light preprocessing and then generate the gensim dictionary and corpus. Each corpus item document a series of tuples (1st representing tokenid from dictionary and 2nd representing token frequency in the document
# Import Dictionary
from gensim.corpora.dictionary import Dictionary

# Create a Dictionary from the articles: dictionary
dictionary = Dictionary(articles)

# Select the id for "computer": computer_id
computer_id = dictionary.token2id.get("computer")

# Use computer_id with the dictionary to print the word
print(dictionary.get(computer_id))

# Create a MmCorpus: corpus
corpus = [dictionary.doc2bow(article) for article in articles]

# Print the first 10 word ids with their frequency counts from the fifth document
print(corpus[4][:10])
=> Output: <script.py> output:
    computer
    [(0, 85), (8, 11), (10, 2), (25, 1), (27, 2), (41, 33), (42, 1), (43, 1), (44, 1), (45, 3)]

d) Gensim bag-of-words
Now, you'll use your new gensim corpus and dictionary to see the most common terms per document and across all documents. You can use your dictionary to look up the terms. Take a guess at what the topics are and feel free to explore more documents in the IPython Shell! You have access to the dictionary and corpus objects you created in the previous exercise, as well as the Python defaultdict and itertools to help with the creation of intermediate data structures for analysis.

defaultdict allows us to initialize a dictionary that will assign a default value to non-existent keys. By supplying the argument int, we are able to ensure that any non-existent keys are automatically assigned a default value of 0. This makes it ideal for storing the counts of words in this exercise.

itertools.chain.from_iterable() allows us to iterate through a set of sequences as if they were one continuous sequence. Using this function, we can easily iterate through our corpus object (which is a list of lists).

The fifth document from corpus is stored in the variable doc, which has been sorted in descending order.

# Save the fifth document: doc
doc = corpus[4]

# Sort the doc for frequency: bow_doc
bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)

# Print the top 5 words of the document alongside the count
for word_id, word_count in bow_doc[:5]:
    print(dictionary.get(word_id), word_count)
    
# Create the defaultdict: total_word_count
total_word_count = defaultdict(int)
for word_id, word_count in itertools.chain.from_iterable(corpus):
    total_word_count[word_id] += word_count

# Create a sorted list from the defaultdict: sorted_word_count 
sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) 

# Print the top 5 words across all documents alongside the count
for word_id, word_count in sorted_word_count[:5]:
    print(dictionary.get(word_id), word_count)
=> Output: <script.py> output:
    engineering 91
    '' 85
    reverse 73
    software 51
    `` 33
    '' 1006
    computer 598
    `` 555
    software 450
    cite 322

e)
