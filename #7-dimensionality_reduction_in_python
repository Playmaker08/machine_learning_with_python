## CHAPTER 1: EXPLORING HIGH DIMENSIONAL DATA

a) Identify and remove features without variance from the Pokémon dataset

# Leave this list as is
number_cols = ['HP', 'Attack', 'Defense']

# Remove the feature without variance from this list
non_number_cols = ['Name', 'Type']

# Create a new DataFrame by subselecting the chosen features
df_selected = pokemon_df[number_cols + non_number_cols]

# Prints the first 5 lines of the new DataFrame
print(df_selected.head())

b) Visually detect redundant features using Seaborn pairplots

# Create a pairplot and color the points using the 'Gender' feature
sns.pairplot(ansur_df_1, hue='Gender', diag_kind='hist')

# Show the plot
plt.show()

# Remove one of the redundant features 
reduced_df = ansur_df_1.drop('body_height', axis=1)

# Create a pairplot and color the points using the 'Gender' feature
sns.pairplot(reduced_df, hue='Gender')

# Show the plot
plt.show()

# Create a pairplot and color the points using the 'Gender' feature
sns.pairplot(ansur_df_2, hue='Gender', diag_kind='hist')

# Show the plot
plt.show()

# Remove the redundant feature
reduced_df = ansur_df_2.drop('n_legs', axis=1)

# Create a pairplot and color the points using the 'Gender' feature
sns.pairplot(reduced_df, hue='Gender', diag_kind='hist')

# Show the plot
plt.show()

c) Fitting t-SNE to the ANSUR dataset

# Non-numerical columns in the dataset
non_numeric = ['Branch', 'Gender', 'Component']

# Drop the non-numerical columns from df
df_numeric = df.drop(non_numeric, axis=1)

# Create a t-SNE model with learning rate 50
m = TSNE(learning_rate=50)

# Fit and transform the t-SNE model on the numeric dataset
tsne_features = m.fit_transform(df_numeric)
print(tsne_features.shape)
// Output: t-SNE reduced the more than 90 features in the dataset to just 2 which you can now plot.

d) Visualizing the t-SNE dimensionality reduction on the ANSUR dataset

# Color the points according to Army Component
sns.scatterplot(x="x", y="y", hue='Component', data=df)

# Show the plot
plt.show()

# Color the points by Army Branch
sns.scatterplot(x="x", y="y", hue='Branch', data=df)

# Show the plot
plt.show()

# Color the points by Gender
sns.scatterplot(x="x", y="y", hue='Gender', data=df)

# Show the plot
plt.show()


## CHAPTER 2: FEATURE SELECTION I - SELECTING FOR FEATURE INFORMATION

a) Splitting the ANSUR dataset into training and test sets

# Import train_test_split()
from sklearn.model_selection import train_test_split

# Select the Gender column as the feature to be predicted (y)
y = ansur_df['Gender']

# Remove the Gender column to create the training data
X = ansur_df.drop('Gender', axis=1)

# Perform a 70% train and 30% test data split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

print(f"{X_test.shape[0]} rows in test set vs. {X_train.shape[0]} in training set, {X_test.shape[1]} Features.")

// Output: 60 rows in test set vs. 140 in training set, 92 Features.

b) Fitting and testing an SVM classifier on the ANSUR dataset

# Import SVC from sklearn.svm and accuracy_score from sklearn.metrics
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Create an instance of the Support Vector Classification class
svc = SVC()

# Fit the model to the training data
svc.fit(X_train, y_train)

# Calculate accuracy scores on both train and test data
accuracy_train = accuracy_score(y_train, svc.predict(X_train))
accuracy_test = accuracy_score(y_test, svc.predict(X_test))

print(f"{accuracy_test:.1%} accuracy on test set vs. {accuracy_train:.1%} on training set")
// Output: 81.7% accuracy on test set vs. 88.6% on training set

c) Reducing overfitting using dimensionality reduction

# Assign just the 'neckcircumferencebase' column from ansur_df to X
X = ansur_df[['neckcircumferencebase']]

# Split the data, instantiate a classifier and fit the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
svc = SVC()
svc.fit(X_train, y_train)

# Calculate accuracy scores on both train and test data
accuracy_train = accuracy_score(y_train, svc.predict(X_train))
accuracy_test = accuracy_score(y_test, svc.predict(X_test))

print(f"{accuracy_test:.1%} accuracy on test set vs. {accuracy_train:.1%} on training set")
// Output: 95.0% accuracy on test set vs. 95.0% on training set

d) Finding a good variance threshold

# Create the boxplot
head_df.boxplot()
plt.show()

# Normalize the data
normalized_df = head_df / head_df.mean()
normalized_df.boxplot()
plt.show()

# Print the variances of the normalized data
print(normalized_df.var())

// Output: 
    headbreadth          1.679e-03
    headcircumference    1.030e-03
    headlength           1.868e-03
    tragiontopofhead     2.640e-03
    n_hairs              1.003e-08
    measurement_error    0.000e+00
    dtype: float64
A threshold of 1.0e-03 (0.001) will remove the two low variance features.

e) Removing low-variance features using VarianceThreshold

from sklearn.feature_selection import VarianceThreshold

# Create a VarianceThreshold feature selector with threshold 0.001
sel = VarianceThreshold(threshold=0.001)

# Fit the selector to normalized head_df
sel.fit(head_df / head_df.mean())

# Create a boolean mask
mask = sel.get_support()

# Apply the mask to create a reduced DataFrame
reduced_df = head_df.loc[:, mask]

print(f"Dimensionality reduced from {head_df.shape[1]} to {reduced_df.shape[1]}.")
// Output: Dimensionality reduced from 6 to 4.

f) Removing features with many missing values

# Create a boolean mask on whether each feature less than 50% missing values.
mask = school_df.isna().sum() / len(school_df) < 0.5

# Create a reduced dataset by applying the mask
reduced_df = school_df.loc[:, mask]

print(school_df.shape)
print(reduced_df.shape)
// Output: The number of features went down from 21 to 19.

g) Removing redundant values and visualizing the matrix using seaborn

# Create the correlation matrix
corr = ansur_df.corr()

# Draw a heatmap of the correlation matrix
sns.heatmap(corr,  cmap=cmap, center=0, linewidths=1, annot=True, fmt=".2f")
plt.show()

# Create the correlation matrix
corr = ansur_df.corr()

# Generate a mask for the upper triangle 
mask = np.triu(np.ones_like(corr, dtype=bool))
print(mask)

# Add the mask to the heatmap
sns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=1, annot=True, fmt=".2f")
plt.show()

h) Filtering out highly correlated features from a dataset using a correlation matrix

# Calculate the correlation matrix and take the absolute value
corr_df = ansur_df.corr().abs()

# Create a True/False mask and apply it
mask = np.triu(np.ones_like(corr_df, dtype=bool))
tri_df = corr_df.mask(mask)

# List column names of highly correlated features (r > 0.95)
to_drop = [c for c in tri_df.columns if any(tri_df[c] > 0.95)]

# Drop the features in the to_drop list
reduced_df = ansur_df.drop(to_drop, axis=1)

print(f"The reduced_df DataFrame has {reduced_df.shape[1]} columns.")

i) CASE: Nuclear Energy and Pool Drownings

# Print the first five lines of weird_df
print(weird_df.head())

# Put nuclear energy production on the x-axis and the number of pool drownings on the y-axis
sns.scatterplot(x="nuclear_energy", y="pool_drownings", data=weird_df)
plt.show()

# Print out the correlation matrix of weird_df
print(weird_df.corr())
// Output:
                    pool_drownings  nuclear_energy
    pool_drownings           1.000           0.901
    nuclear_energy           0.901           1.000
=> Not much to conclude because correlation does not imply causation 


## CHAPTER 3: FEATURE SELECTION II - SELECTING FOR MODEL ACCURACY

// Notes: A "feature mask" in machine learning is a binary vector or matrix that indicates which features in a dataset should be considered by the model, essentially acting as a filter to selectively include or exclude certain features during training and prediction, allowing the model to focus on the most relevant information and improve its performance; it's often used in feature selection techniques to identify important features and reduce dimensionality. 


a) Train a Logistic Regression model to predict whether a person has diabetes

# Fit the scaler on the training features and transform these in one go
X_train_std = scaler.fit_transform(X_train)

# Fit the logistic regression model on the scaled training data
lr.fit(X_train_std, y_train)

# Scale the test features
X_test_std = scaler.transform(X_test)

# Predict diabetes presence on the scaled test set
y_pred = lr.predict(X_test_std)

# Prints accuracy metrics and feature coefficients
print(f"{accuracy_score(y_test, y_pred):.1%} accuracy on test set.")
print(dict(zip(X_train.columns, abs(lr.coef_[0]).round(2))))
// Output:     79.6% accuracy on test set.
    {'pregnant': 0.05, 'glucose': 1.23, 'diastolic': 0.03, 'triceps': 0.24, 'insulin': 0.19, 'bmi': 0.38, 'family': 0.35, 'age': 0.34}

b) Reduce the number of features in the diabetes classifier while maintaining accuracy using Manual Recursive Feature Elimination (RFE)

Default features: X = diabetes_df[['pregnant', 'glucose', 'diastolic', 'triceps', 'insulin', 'bmi', 'family', 'age']]

//Output from running the initial code (1):
    79.6% accuracy on test set.
    {'pregnant': 0.05, 'glucose': 1.23, 'diastolic': 0.03, 'triceps': 0.24, 'insulin': 0.19, 'bmi': 0.38, 'family': 0.35, 'age': 0.34}

# Remove the feature with the lowest model coefficient
X = diabetes_df[['pregnant', 'glucose', 'triceps', 'insulin', 'bmi', 'family', 'age']]

# Performs a 25-75% train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Scales features and fits the logistic regression model
lr.fit(scaler.fit_transform(X_train), y_train)

# Calculates the accuracy on the test set and prints coefficients
acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))
print(f"{acc:.1%} accuracy on test set.") 
print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))

// Output (2):
    80.6% accuracy on test set.
    {'pregnant': 0.05, 'glucose': 1.24, 'triceps': 0.24, 'insulin': 0.2, 'bmi': 0.39, 'family': 0.34, 'age': 0.35}

# Remove the 2 features with the lowest model coefficients
X = diabetes_df[['glucose', 'triceps', 'bmi', 'family', 'age']]

# Performs a 25-75% train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Scales features and fits the logistic regression model
lr.fit(scaler.fit_transform(X_train), y_train)

# Calculates the accuracy on the test set and prints coefficients
acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))
print(f"{acc:.1%} accuracy on test set.") 
print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))

// Output (3):
    79.6% accuracy on test set.
    {'glucose': 1.13, 'triceps': 0.25, 'bmi': 0.34, 'family': 0.34, 'age': 0.37}

# Only keep the feature with the highest coefficient
X = diabetes_df[['glucose']]

# Performs a 25-75% train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Scales features and fits the logistic regression model to the data
lr.fit(scaler.fit_transform(X_train), y_train)

# Calculates the accuracy on the test set and prints coefficients
acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))
print(f"{acc:.1%} accuracy on test set.") 
print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))

// Output (4):
    75.5% accuracy on test set.
    {'glucose': 1.28}

c) Use Recursive Feature Elimination (RFE) to automatically select the top 3 most important features while maintaining accuracy.

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# Create the RFE with a LogisticRegression estimator and 3 features to select
rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)

# Fit the eliminator to the data
rfe.fit(X_train, y_train)

# Print the features and their ranking (high = dropped early on)
print(dict(zip(X.columns, rfe.ranking_)))

# Print the features that are not eliminated
print(X.columns[rfe.support_])

# Calculate the test set accuracy
acc = accuracy_score(y_test, rfe.predict(X_test))
print(f"{acc:.1%} accuracy on test set.")
// Output:
    Fitting estimator with 8 features.
    Fitting estimator with 7 features.
    Fitting estimator with 6 features.
    Fitting estimator with 5 features.
    Fitting estimator with 4 features.
    {'pregnant': 5, 'glucose': 1, 'diastolic': 6, 'triceps': 3, 'insulin': 4, 'bmi': 1, 'family': 2, 'age': 1}
    Index(['glucose', 'bmi', 'age'], dtype='object')
    80.6% accuracy on test set.

d) Train a Random Forest Classifier to predict diabetes, evaluate accuracy, and analyze feature importance.

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Perform a 75% training and 25% test data split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Fit the random forest model to the training data
rf = RandomForestClassifier(random_state=0)
rf.fit(X_train, y_train)

# Calculate the accuracy
acc = accuracy_score(y_test, rf.predict(X_test))

# Print the importances per feature
print(dict(zip(X.columns, rf.feature_importances_.round(2))))

# Print accuracy
print(f"{acc:.1%} accuracy on test set.")
// Output:     {'pregnant': 0.07, 'glucose': 0.25, 'diastolic': 0.09, 'triceps': 0.09, 'insulin': 0.14, 'bmi': 0.12, 'family': 0.12, 'age': 0.13}
    79.6% accuracy on test set.

e) Identify the most important features using the trained Random Forest model (rf).

# Create a mask for features importances above the threshold
mask = rf.feature_importances_ > 0.15

# Prints out the mask
print(mask)
// Output:     [False  True False False False False False  True]

# Apply the mask to the feature dataset X
reduced_X = X.loc[:, mask]

# prints out the selected column names
print(reduced_X.columns)
// Output:     Index(['glucose', 'age'], dtype='object')

f) Use Recursive Feature Elimination (RFE) with a Random Forest model to iteratively remove less important features.

# Wrap the feature eliminator around the random forest model
rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, verbose=1)

# Fit the model to the training data
rfe.fit(X_train, y_train)

# Create a mask using the support_ attribute of rfe
mask = rfe.support_

# Apply the mask to the feature dataset X and print the result
reduced_X = X.loc[:, mask]
print(reduced_X.columns)

# Set the feature eliminator to remove 2 features on each step
rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)
// Output:
    Fitting estimator with 8 features.
    Fitting estimator with 6 features.
    Fitting estimator with 4 features.
    Index(['glucose', 'insulin'], dtype='object')

g) Train a LASSO regression model to predict Body Mass Index (BMI) while ensuring features are standardized

from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split

# Set the test size to 30% to get a 70-30% train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Fit the scaler on the training features and transform these in one go
X_train_std = scaler.fit_transform(X_train)

# Create the Lasso model
la = Lasso()

# Fit it to the standardized training data
la.fit(X_train_std, y_train)

h) Evaluate the performance of the trained LASSO regression model by calculating R² and number of features ignored.

# Transform the test set with the pre-fitted scaler
X_test_std = scaler.transform(X_test)

# Calculate the coefficient of determination (R squared) on X_test_std
r_squared = la.score(X_test_std, y_test)
print(f"The model can predict {r_squared:.1%} of the variance in the test set.")

# Create a list that has True values when coefficients equal 0
zero_coef = la.coef_ == 0

# Calculate how many features have a zero coefficient
n_ignored = sum(zero_coef)
print(f"The model has ignored {n_ignored} out of {len(la.coef_)} features.")
// Output:
    The model can predict 84.7% of the variance in the test set.
    The model has ignored 82 out of 91 features.

i) Find the highest value of alpha that keeps the R² score above 98% while minimizing ignored features

# Find the highest alpha value with R-squared above 98%
la = Lasso(alpha=0.1, random_state=0)

# Fits the model and calculates performance stats
la.fit(X_train_std, y_train)
r_squared = la.score(X_test_std, y_test)
n_ignored_features = sum(la.coef_ == 0)

# Print peformance stats 
print(f"The model can predict {r_squared:.1%} of the variance in the test set.")
print(f"{n_ignored_features} out of {len(la.coef_)} features were ignored.")
// Output:
    The model can predict 98.3% of the variance in the test set.
    64 out of 91 features were ignored.

j) Use LassoCV() to automatically find the optimal alpha (regularization strength) for predicting biceps circumference.

from sklearn.linear_model import LassoCV

# Create and fit the LassoCV model on the training set
lcv = LassoCV(cv=5, random_state=0)  # 5-fold cross-validation
lcv.fit(X_train, y_train)
print(f'Optimal alpha = {lcv.alpha_:.3f}')

# Calculate R squared on the test set
r_squared = lcv.score(X_test, y_test)
print(f'The model explains {r_squared:.1%} of the test set variance')

# Create a mask for coefficients not equal to zero
lcv_mask = lcv.coef_ != 0
print(f'{sum(lcv_mask)} features out of {len(lcv_mask)} selected')
// Output:
    Optimal alpha = 0.097
    The model explains 87.4% of the test set variance
    22 features out of 32 selected

k) Use Recursive Feature Elimination (RFE) with a Gradient Boosting Regressor to select the top 10 most important features.

from sklearn.feature_selection import RFE
from sklearn.ensemble import GradientBoostingRegressor

# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step
rfe_gb = RFE(estimator=GradientBoostingRegressor(), 
             n_features_to_select=10, step=3, verbose=1)
rfe_gb.fit(X_train, y_train)

# Calculate the R squared on the test set
r_squared = rfe_gb.score(X_test, y_test)
print(f'The model can explain {r_squared:.1%} of the variance in the test set')

# Assign the support array to gb_mask
gb_mask = rfe_gb.support_
// Output:
    Fitting estimator with 32 features.
    Fitting estimator with 29 features.
    Fitting estimator with 26 features.
    Fitting estimator with 23 features.
    Fitting estimator with 20 features.
    Fitting estimator with 17 features.
    Fitting estimator with 14 features.
    Fitting estimator with 11 features.
   The model can explain 85.2% of the variance in the test set

from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor

# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step
rfe_rf = RFE(estimator=RandomForestRegressor(), 
             n_features_to_select=10, step=3, verbose=1)
rfe_rf.fit(X_train, y_train)

# Calculate the R squared on the test set
r_squared = rfe_rf.score(X_test, y_test)
print(f'The model can explain {r_squared:.1%} of the variance in the test set')

# Assign the support array to rf_mask
rf_mask = rfe_rf.support_
// Output:
    Fitting estimator with 32 features.
    Fitting estimator with 29 features.
    Fitting estimator with 26 features.
    Fitting estimator with 23 features.
    Fitting estimator with 20 features.
    Fitting estimator with 17 features.
    Fitting estimator with 14 features.
    Fitting estimator with 11 features.
    The model can explain 84.4% of the variance in the test set

l) Combine feature selection results from three different models (LassoCV, Random Forest, Gradient Boosting) to create a meta feature mask, 
then reduce dimensionality and evaluate model performance.

# Sum the votes of the three models
votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)
// Output:  [0 0 3 3 0 1 0 3 1 0 1 3 1 1 0 0 1 2 1 1 1 1 3 2 1 3 2 1 1 1 1 3]

# Create a mask for features selected by all 3 models
meta_mask = votes == 3
// Output:     [False False  True  True False False False  True False False False  True
     False False False False False False False False False False  True False
     False  True False False False False False  True]

# Apply the dimensionality reduction on X
X_reduced = X.loc[:, meta_mask]
// Output:     Index(['bideltoidbreadth', 'buttockcircumference', 'chestcircumference', 'forearmcircumferenceflexed', 'shouldercircumference', 'thighcircumference', 'BMI'], dtype='object')

# Plug the reduced dataset into a linear regression pipeline
X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)
lm.fit(scaler.fit_transform(X_train), y_train)
r_squared = lm.score(scaler.transform(X_test), y_test)
print(f'The model can explain {r_squared:.1%} of the variance in the test set using {len(lm.coef_)} features.')
// Output:     The model can explain 86.7% of the variance in the test set using 7 features.


## CHAPTER 4: FEATURE EXTRACTION

a) Transform the dataset to include product price by calculating it from quantity sold and total revenue, then drop unnecessary columns.

# Calculate the price from the quantity sold and revenue
sales_df['price'] = sales_df['revenue'] / sales_df['quantity']

# Drop the quantity and revenue features
reduced_df = sales_df.drop(['quantity', 'revenue'], axis=1)

print(reduced_df.head())
// Output:
  storeID  product  quantity  revenue
0       A   Apples      1811   9300.6
1       A  Bananas      1003   3375.2
2       A  Oranges      1604   8528.5
3       B   Apples      1785   9181.0
4       B  Bananas       944   3680.2

<script.py> output:
      storeID  product  price
    0       A   Apples  5.136
    1       A  Bananas  3.365
    2       A  Oranges  5.317
    3       B   Apples  5.143
    4       B  Bananas  3.899

b) Compute the mean height from three height measurements and drop the original columns.

# Calculate the mean height
height_df['height'] = height_df[['height_1', 'height_2', 'height_3']].mean(axis=1)

# Drop the 3 original height features
reduced_df = height_df.drop(['height_1', 'height_2', 'height_3'], axis=1)

print(reduced_df.head())

c) Visually inspect a 4-feature sample of the ANSUR dataset before and after PCA using Seaborn's pairplot().

# Create a pairplot to inspect ansur_df
sns.pairplot(ansur_df)
plt.show()

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Create the scaler and standardize the data
scaler = StandardScaler()
ansur_std = scaler.fit_transform(ansur_df)

# Create the PCA instance and fit and transform the data with pca
pca = PCA()
pc = pca.fit_transform(ansur_std)

# This changes the numpy array output back to a DataFrame
pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])

# Create a pairplot of the principal component DataFrame
sns.pairplot(pc_df)
plt.show()

d) Apply Principal Component Analysis (PCA) on a 13-dimensional ANSUR dataset.

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Scale the data
scaler = StandardScaler()
ansur_std = scaler.fit_transform(ansur_df)

# Apply PCA
pca = PCA()
pca.fit(ansur_std)

e) Inspect the explained variance ratio of each Principal Component in the PCA model.

# Inspect the explained variance ratio per component
print(pca.explained_variance_ratio_)
// Output: 
    [0.61449404 0.19893965 0.06803095 0.03770499 0.03031502 0.0171759
     0.01072762 0.00656681 0.00634743 0.00436015 0.0026586  0.00202617
     0.00065268]
=> The 4th principal component explains 3.77% variance

# Print the cumulative sum of the explained variance ratio
print(pca.explained_variance_ratio_.cumsum())
// Output:
    [0.61449404 0.81343368 0.88146463 0.91916962 0.94948464 0.96666054
     0.97738816 0.98395496 0.99030239 0.99466254 0.99732115 0.99934732
     1.        ]
=> Using just 4 principal components we can explain more than 90% of the variance in the 13 feature dataset.

f) Apply PCA to the numeric features of the Pokemon dataset (poke_df) using a Pipeline that scales the data and reduces dimensions.

# Build the pipeline
pipe = Pipeline([('scaler', StandardScaler()),
        		 ('reducer', PCA(n_components=2))])

# Fit it to the dataset and extract the component vectors
pipe.fit(poke_df)
vectors = pipe['reducer'].components_.round(2)

# Print feature effects
print('PC 1 effects = ' + str(dict(zip(poke_df.columns, vectors[0]))))
print('PC 2 effects = ' + str(dict(zip(poke_df.columns, vectors[1]))))
// Output:
    PC 1 effects = {'HP': 0.39, 'Attack': 0.44, 'Defense': 0.36, 'Sp. Atk': 0.46, 'Sp. Def': 0.45, 'Speed': 0.34}
    PC 2 effects = {'HP': 0.08, 'Attack': -0.01, 'Defense': 0.63, 'Sp. Atk': -0.31, 'Sp. Def': 0.24, 'Speed': -0.67}

# Fit the pipeline to poke_df and transform the data
pc = pipe.fit_transform(poke_df)
print(pc)
// Output: 
    [[-1.5563747  -0.02148212]
     [-0.36286656 -0.05026854]
     [ 1.28015158 -0.06272022]
     ...
     [ 2.45821626 -0.51588158]
     [ 3.5303971  -0.95106516]
     [ 2.23378629  0.53762985]]

# Add the 2 components to poke_cat_df
poke_cat_df['PC 1'] = pc[:, 0]
poke_cat_df['PC 2'] = pc[:, 1]
print(poke_cat_df.head())
// Output:
        Type  Legendary   PC 1   PC 2
    0  Grass      False -1.556 -0.021
    1  Grass      False -0.363 -0.050
    2  Grass      False  1.280 -0.063
    3  Grass      False  2.621  0.704
    4   Fire      False -1.758 -0.706

# Use the Type feature to color the PC 1 vs. PC 2 scatterplot
sns.scatterplot(data=poke_cat_df, x='PC 1', y='PC 2', hue='Type')
plt.show()

# Use the Legendary feature to color the PC 1 vs. PC 2 scatterplot
sns.scatterplot(data=poke_cat_df, x='PC 1', y='PC 2', hue='Legendary')
plt.show()

g) Build a pipeline that scales the data, applies PCA (2 components), and classifies Legendary Pokémon using a Random Forest Classifier.

# Build the pipeline
pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('reducer', PCA(n_components=2)),
        ('classifier', RandomForestClassifier(random_state=0))])

# Fit the pipeline to the training data
pipe.fit(X_train, y_train)

# Prints the explained variance ratio
print(pipe['reducer'].explained_variance_ratio_)
// Output:    [0.45624044 0.17767414]

# Score the accuracy on the test set
accuracy = pipe.score(X_test, y_test)

# Prints the model accuracy
print(f'{accuracy:.1%} test set accuracy')
// Output: 95.4% test set accuracy

# Prints the explained variance ratio and accuracy
print(pipe['reducer'].explained_variance_ratio_)
print(f'{accuracy:.1%} test set accuracy')
// Output:     [0.45624044 0.17767414 0.12858833]
    94.6% test set accuracy
=> Looks like adding the third component does not increase the model accuracy, even though it adds information to the dataset.

h) Let PCA determine the number of components to calculate based on an explained variance threshold that you decide.

# Pipe a scaler to PCA selecting 80% of the variance
pipe = Pipeline([('scaler', StandardScaler()),
        		 ('reducer', PCA(n_components=0.8))])

# Fit the pipe to the data
pipe.fit(ansur_df)
print(f'{len(pipe["reducer"].components_)} components selected')
// Output: 11 components selected

# Let PCA select 90% of the variance
pipe = Pipeline([('scaler', StandardScaler()),
        		 ('reducer', PCA(n_components=0.9))])

# Fit the pipe to the data
pipe.fit(ansur_df)
print(f'{len(pipe["reducer"].components_)} components selected')
// Output: 23 components selected
=> 12 additional features needed to explain 90% instead of 80% of the variance?

i) Make a decision on the number of principal components to reduce your data to using the "elbow in the plot" technique.

# Pipeline a scaler and pca selecting 10 components
pipe = Pipeline([('scaler', StandardScaler()),
        		 ('reducer', PCA(n_components=10))])

# Fit the pipe to the data
pipe.fit(ansur_df)

# Plot the explained variance ratio
plt.plot(pipe['reducer'].explained_variance_ratio_)

plt.xlabel('Principal component index')
plt.ylabel('Explained variance ratio')
plt.show()

j) Reduce the size of 16 images with hand written digits (MNIST dataset) using PCA.

# Plot the MNIST sample data
plot_digits(X_test)

# Transform the input data to principal components
pc = pipe.transform(X_test)

# Prints the number of features per dataset
print(f"X_test has {X_test.shape[1]} features")
print(f"pc has {pc.shape[1]} features")
// Output:
    X_test has 784 features
    pc has 78 features

# Inverse transform the components to original feature space
X_rebuilt = pipe.inverse_transform(pc)

# Prints the number of features
print(f"X_rebuilt has {X_rebuilt.shape[1]} features")
// Output:    X_rebuilt has 784 features

# Plot the reconstructed data
plot_digits(X_rebuilt)
